{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenización"
      ],
      "metadata": {
        "id": "UVXsD3cYU1PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Tokenización por palabras\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "texto = \"Hola, esto es una prueba de tokenización por palabras.\"\n",
        "\n",
        "tokens = word_tokenize(texto)\n",
        "print(\"Tokens de palabras:\", tokens)"
      ],
      "metadata": {
        "id": "i_A8M8aZOWST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Tokenización por subpalabras\n",
        "\n",
        "from transformers import LlamaTokenizerFast\n",
        "\n",
        "tokenizer = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
        "texto = \"Hola, esto es una prueba de tokenización por subpalabras.\"\n",
        "\n",
        "# Tokenizar con BPE\n",
        "tokens = tokenizer.tokenize(texto)\n",
        "print(\"Tokens de subpalabras (BPE)\", tokens)"
      ],
      "metadata": {
        "id": "RAL9_6jjOokZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Tokenización por frase\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "texto = \"Hola, esto es una prueba de tokenización por frase. ¿Cómo estás?.\"\n",
        "\n",
        "frases = sent_tokenize(texto, language='spanish')\n",
        "print(\"Tokens de frases:\", frases)"
      ],
      "metadata": {
        "id": "0Z7HfwsESesf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorización"
      ],
      "metadata": {
        "id": "Ask7aO67U6Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Vectorización por Bag of words\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "textos = [\"Hola, hola. Esto es una prueba, es la prueba definitiva.\", \"Vaya prueba, repetiste hola.\"]\n",
        "vectorizador = CountVectorizer()\n",
        "X = vectorizador.fit_transform(textos)\n",
        "print(vectorizador.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "7aQei59FUzU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Vectorización por TF-IDF\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizador = TfidfVectorizer()\n",
        "X = vectorizador.fit_transform(textos)\n",
        "print(vectorizador.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "sTv6Zpj7VZLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Vectorización por word embeddings\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "tokens = [word_tokenize(texto) for texto in textos]\n",
        "modelo = Word2Vec(tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "print(modelo.wv['prueba'])"
      ],
      "metadata": {
        "id": "IQ2CxY_VWZ5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LlamaTokenizerFast\n",
        "\n",
        "tokenizer = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
        "embeddings = tokenizer.encode(\"Hola, hola. Esto es una prueba, es la prueba definitiva.\")\n",
        "print(embeddings)"
      ],
      "metadata": {
        "id": "XqC6tnCUZ0Ow"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}